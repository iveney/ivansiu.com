<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: machine learning | Ivan's Blog]]></title>
  <link href="http://blog.ivansiu.com/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://blog.ivansiu.com/"/>
  <updated>2021-01-15T16:59:58-08:00</updated>
  <id>http://blog.ivansiu.com/</id>
  <author>
    <name><![CDATA[Zigang "Ivan" Xiao]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Recommendation System Review]]></title>
    <link href="http://blog.ivansiu.com/blog/2017/09/04/recommendation-system-review/"/>
    <updated>2017-09-04T15:25:06-07:00</updated>
    <id>http://blog.ivansiu.com/blog/2017/09/04/recommendation-system-review</id>
    <content type="html"><![CDATA[A pretty good review for recsys. Goes over content-based and collaborative filtering (item-based and model-based) approach.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes on Maximum Likelihood, Maximum a Posteriori and Naive Bayes]]></title>
    <link href="http://blog.ivansiu.com/blog/2014/09/24/notes-on-maximum-likelihood/"/>
    <updated>2014-09-24T15:41:44-05:00</updated>
    <id>http://blog.ivansiu.com/blog/2014/09/24/notes-on-maximum-likelihood</id>
    <content type="html"><![CDATA[Let $\data$ be a set of data generated from some distribution parameterized by
$\theta$. We want to *estimate* the unknown parameter $\theta$. What we can do?

<!-- more -->

Essentially, we want to find a most likely value of $\theta$ given $\data$,
that is $\arg \max P(\theta | \data)$. According to Bayes Rule, we have

$$
P(\theta \given \data) = \frac{P(\data \given \theta)P(\theta)}{P(\data)}
$$

and the terms have the following meanings:

- $P(\theta \given \data)$: Posterior
- $P(\data \given \theta)$: Likelihood
- $P(\theta)$: Prior
- $P(\data)$: Evidence

## Maximum Likelihood Estimation (MLE)

An easy way out is to use the MLE method.
We want to find a $\theta$ the *best explains* the data.
That is, we maximize $P(\data \given \theta)$.
Denote such a value as $\hat{\theta}_{ML}$. We have

$$
\hat{\theta}_{ML} = \argmax_\theta P(\data \given \theta) =
\argmax_\theta P(\mathbf{x}_1, \ldots, \mathbf{x}_N \given \theta )
$$

Note that the above $P$ is a joint distribution over the data.
We usually assume the observations are *independent*. Thus, we have

$$
P(\mathbf{x}_1, \ldots, \mathbf{x}_N \given \theta ) =
\prod_{i=1}^{N} P(\mathbf{x}_i \given \theta )
$$

We usually use logarithm to simplify the computation, as
logarithm is monotonically increasing. Thus, we write:

$$
\mathcal{L}(\data \given \theta) = \sum_{i=1}^N \log P(\mathbf{x}_i \given \theta )
$$

Finally, we seek for the ML solution:

$$
\hat{\theta}_{ML} = \argmax_\theta \mathcal{L}(\data \given \theta)
$$

If we know the distribution $P$, we can usually solve the above by
setting derivative of $\theta$ to 0 and solve for $\theta$, that is,

$$
\frac{\partial L}{\partial \theta} = 0
$$

## Maximum A Posteriori (MAP)

In MAP, we maximize $P(\theta \given \data)$ directly. Denote the MAP hypothesis
as $\hat{\theta}_{MAP}$, we have:

$$\begin{array}{rl}
\hat{\theta}_{MAP} = & \argmax_\theta P(\theta \given \data) \\\\
 = & \argmax_\theta \frac{P(\data \given \theta)P(\theta)}{P(\data)} \\\\
 = & \argmax_\theta P(\data \given \theta)P(\theta)
\end{array}$$

Note that the last step is due to the evidence (data) $\data$ is constant, and
thus can be omitted in $\argmax$.

At this step, we notice that the only difference between $\hat{\theta}_{ML}$ and
$\hat{\theta}_{MAP}$ is the prior term $P(\theta)$. Another way to interpret
is that we consider $MAP$ is more general than $MLE$, as if we assume all
the possible $\theta$ are equally probable a priori, e.g., they have the same
prior probability, or *uniform prior*, we can effectively remove $P(\theta)$
from the MAP formula, and it looks like exactly the same as MLE.

Finally, if the independent observation holds, again we can use logarithm and
expand $\hat{\theta}_{MAP}$ as:

$$
\begin{array}{rl}
\hat{\theta}_{MAP} = & \argmax_\theta L(\data \given \theta) \\\\
 = & \argmax_\theta \sum_{i=1}^{N} \log P(\mathbf{x}_i \given \theta ) + \log P(\theta)
\end{array}
$$

The extra prior term has the effect that we are essentially 'pulling' the
$\theta$ distribution towards prior value. This makes sense as we are
putting our domain knowledge as *prior* and intuitively the estimation is
biased towards the *prior* value.

## Naive Bayes Classifier

Assume that we are given a set of data $\data$, where each example
$\mathbf{x_j}=(a_1, a_2, \ldots, a_n)$,  which can be viewed as *conjunctions of
attributes values*. $v_j \in V$ is the corresponding class value. Using MAP, we
can classify an example $\mathbf{x}$ as:

$$v_{MAP}=\argmax_{v_j\in V} P(v_j \given a_1, \ldots, a_n)$$

The problem is that it is hard to find a joint distribution for  $P(\mathbf{x}
\given \theta)$. If we use the data to estimate the distribution, we typically
don't have enough data for each attribute. In other words, the  data we have is
very sparse compared to the whole distribution space.

Naive bayes makes the assumption that each
attribute is *conditionally independent* given the target class $v_j$, that is,

$$P(a_1, \ldots, a_n \given v_j) = \prod_{i=1}^n P(a_i \given v_j)$$

which can be easily estimated from the data.
Thus, we have the following naive bayes classifier:

$$v_{NB} = \argmax_{v_j \in V} P(v_j) \prod_{i=1}^n P(a_i \given v_j)$$

Note that the learning of naive bayes simply involves in estimating
$P(a_i \given v_j)$ and $P(v_j)$ based on the frequencies in the training data.

Normally the conditional independence assumption does not hold, but naive bayes
performs well even if so.
More importantly, **when conditional independence is satisfied, Naive Bayes corresponds to MAP classification.**

## Conclusion

MLE, MAP and Naive Bayes are all connected. While MLE and MAP are parameter
estimation methods that returns a single value of the paramter being estimated,
NB is a classifier that predicts the probability of the class that an example
belongs to. We also have the following insightes:

- Given the data, MLE considers the paramter to be a constant and estimates
a value that provide maximum support for the data.
- MLE does not allow us to 'inject' our beliefs about the likely values for the parameter (prior) in the estimation process.
- MAP allows the fact that the paramter can take values from a prior
(non-uniform) distribution that express our prior beliefs regarding the paramters.
- MAP returns paramter value where the probability is highest given data.
- Again, both MLE and MAP returns a single and specific value for the paramter.
By contrast, *bayesian estimation* computes the full posterior distribution
$P(\theta \given \data)$.


## Thoughts

After reading this [article], I have the following interpretation:

- The Maximum Likelihood approach can be roughly regarded as traditional "frequentist" thinking.
- The MAP approach is a direct applicatin of Bayes Theorem. Thus, it can be regarded as a "bayesian" way of thinking.


[article]: http://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sparse Image Reconstruction via L1-minimization]]></title>
    <link href="http://blog.ivansiu.com/blog/2014/05/19/sparse-image-reconstruction-via-l1-minimization/"/>
    <updated>2014-05-19T15:12:36-05:00</updated>
    <id>http://blog.ivansiu.com/blog/2014/05/19/sparse-image-reconstruction-via-l1-minimization</id>
    <content type="html"><![CDATA[---
nocite: 
  '@candes2005l1, @hesthavensparse, @pant2013new, @candes2006robust'
---


--------------------  -------------------------------  -------------------------
  ![phantom_orig]\      ![phantom_backproj]\              ![phantom_tv]\ 
    Original           Minimum Energy Reconstruction     Sparse Reconstruction
--------------------  -------------------------------  -------------------------

## Introduction

This is a follow up of the L1-minimization series. The previous two posts are:

1. [A Comparison of Least Square, L2-regularization and L1-regularization][L2 vs L1]
2. [Sparse Signal Reconstruction via L1-minimization][Sparse Signal 1D]
<!-- more -->

We have explored using L1-minimization technique to 
[recover a sparse signal][Sparse Signal 1D]. 
The example shows a 1D example. This post demonsrates  on a 2D example, where
the image is viewed as a signal. This makes sense as we can perform 2D Fourier
Transform in the image, where the basis are a combination of *horizontal* and
*vertical* waves. For a complete introduction to FFT on images, refer to 
[this tutorial][FFT 2D]. Notice that similar to 1D signal, we do not measure
the image directly in time domain, but we do it in the frequency domain.
Concretely, say $x$ is the 2D image collapsed to 1D, and $A \in \reals^{k\times n}$ 
is the measurement matrix, $b$ is the observation, we then have $Ax=b$. 
Usually we will require $k = n$ to obtain an exact solution for $x$ given $A$ 
and $b$. Now, if we use FFT and obtain the frequency coefficients as $\hat{x}$,
we can also perform similar measurements $\hat{A} \hat{x} = \hat{b}$,
and the requirement $k = n$ is the same. In other words, the required samples 
(the information) is *the same*. By using the inverse fourier transform,
we can convert $\hat{x}$ back to $x$. The only difference is that the measurement
$\hat{A}$ is taken in frequency (Fourier) domain. As we can see later, we can
utilize sparse information to reduce $k$.

## Image Gradients and Total Variation

We first introduct the concept of image gradients. For any 2D real image `I`, if
we think about each row as a signal, we can then view the 'difference' between
adjacent pixels as (horizontal) gradient `Gx(I)`, this makes sense since a
sharpe change denotes an edge. Similary, we can define the vertical gradient
`Gy(I)` for columns. Thus, we have

$$Gx(I) = \begin{cases}
I_{i+1, j} - I_{ij} & i < n \\\\ 0 & i = n
\end{cases}
\qquad
Gy(I) = \begin{cases}
I_{i, j+1} - I_{ij} & j < n \\\\ 0 & j = n
\end{cases}$$

where the image size is $n\times n$.

Collectively, the image gradient `G(I)` is defined as the
magnitude (2-norm) of both components:

$$G(I)_{ij} = \sqrt{(Gx(I)_{ij})^2 + (Gy(I)_{ij})^2}$$

The following shows `Gx`, `Gy` and `G` of the phantom image:

-----------------  -----------------  -----------------
  ![phantom_gx]\     ![phantom_gy]\     ![phantom_gI]\ 
    `Gx(I)`             `Gy(I)`             `G(I)`
-----------------  -----------------  -----------------

The *total variation* `TV(I)` of an image is just the sum of 
this discrete gradient at every point.

$$TV(I)= \norm{G(I)}_1 = \sum_{i,j} G(I)_{ij}$$

We notice that $TV(I)$ is just the *L1-norm* of $G(I)$,
which leads us to the following: if we have an image that is sparse in
its image gradients, we can exploit that and use our L1-minimization trick.

## Sparse Gradient Image Reconstruction

The ratio of non-zero elements in `Gx`, `Gy` and `G` of the phantom image
is `0.0711`, `0.0634` and `0.0769`, respectively. These ratios are really 
small - and we consider the gradient as *sparse*.

Let $F: \reals^{n\times n} \to \complex^{n\times n}$ be the FFT operator, 
and $F I$ be the Fourier transform taken on image I.
Define a set $\Omega$ as the $k$ two-dimensional frequencies chosen
according to some sampling pattern from the $n \times n$.
We further define $F_\Omega I: \reals^{n \times n} \to \complex^k$ as the 
$k$ observation taken from the fourier transform of image I.
We can then solve the following optimization problem to recover $I$:

$$\min_I \norm{F_\Omega I - b}^2_2$$

where $F_\Omega$ can be view as the measurement matrix, $b$ is the observation,
and we want to find $I$ such that the *reconstruction cost* (energy) is
minimized.  

However, the above does not quite work. As we can see in the following images,
the *L2-minimization* does a poor job, either for a random measurement or
a radial measurement [@candes2006robust] in Fourier domain.

--------------------  ---------------------  --------------------
  ![M rand]            ![phantom rand bp]     ![phantom rand tv]
 Random measurement     L2-minimization        L1-minimization
--------------------  ---------------------  --------------------

--------------------  ---------------------  --------------------
  ![M radial]          ![phantom_backproj]     ![phantom_tv]
 Radial measurement     L2-minimization       L1-minimization
--------------------  ---------------------  --------------------


To utilize the sparse information, we add a L1-regularization term 
to the above objective function, which yields the following:

$$(TV_1) \quad \min_I \norm{F_\Omega I - b}^2_2 + \lambda TV(I)$$

Without surprise, optimizing the above gives us a *perfect* reconstruction
of the original image. 
It is shown that if there exists a piecewise constant I with sufficiently 
few edges (i.e., $G(I)_{ij}$ is nonzero for only a small number of indices i, j), 
then $(TV_1)$ will recover I exactly.

A heavily commented code example is available in my [github repository][code].
Leave a comment if you have any question.

## Probing Further

Now, take a look at another example `cameraman`, which has the following
gradients (intensity rescaled using matlab's `imagesc`.

------------------  --------------------
  ![cameraman]       ![cameraman_grad]
    Cameraman             Gradient
------------------  --------------------

The following shows the reconstructions (left two are using random measurements,
right two are using radial measurements).

----------------------  ----------------------  -----------------  -----------------
 ![cameraman_rand_bp]    ![cameraman_rand_tv]    ![cameraman_bp]    ![cameraman_tv]
      Rand (L2)             Rand (L1)              Radial (L2)        Radial (L1)
----------------------  ----------------------  -----------------  -----------------

As we can see, the results are not as good. In fact, the non-zero ratio of its
gradient is 0.9928, which is not sparse at all. However, if we plot the histogram
of gradients, we will find that most of the gradient magnitudes are small:

![Gradient Histogram][camera_mag_hist]

In particular, most of them are smaller than 200, which means the number of
'changes' that are larger than 200 is small. In fact, the ratio of
gradient > 200 is only *0.0964*! Thus, there are two possible ways to 
discard these information and get a 'compressed' image that is
sparse in gradients:

1. Use mean-shift algorithm to segment the regions such that they have the
same color intensities. K-means or quantization should achieve a similar 
result, though might not as good as mean-shift.
2. Use image filtering to smooth the image, which can effectively average colors
and discard high frequency information.

*I'll leave these conjectures for furture implementation. For those
intereted, please try them yourself and let me know your results.
If you have any thoughts, do not hesitate to leave a comment.*

## References

For interested readers, the following references will be helpful.

[FFT 2D]: http://www.cs.unm.edu/~brayer/vision/fourier.html
[phantom_orig]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/phantom_orig.png
[phantom_backproj]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/phantom_backproj.png
[phantom_tv]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/phantom_tv.png
[phantom_gx]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/phantom_gx.png
[phantom_gy]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/phantom_gy.png
[phantom_gI]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/phantom_gI.png
[M rand]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/measurement_random.png
[M radial]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/measurement_radial.png
[phantom rand bp]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/phantom_rand_bp.png
[phantom rand tv]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/phantom_rand_tv.png
[cameraman]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/cameraman.png
[cameraman_grad]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/cameraman_grad.png
[cameraman_bp]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/cameraman_bp.png
[cameraman_tv]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/cameraman_tv.png
[cameraman_rand_bp]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/cameraman_rand_bp.png
[cameraman_rand_tv]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/cameraman_rand_tv.png
[camera_mag_hist]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/camera_mag_hist.png
[code]: https://github.com/iveney/l1min/blob/master/image_recovery_l1.m

[L2 vs L1]: /blog/2014/05/14/a-comparison-of-least-square-l2-regularization-and-l1-regularization/
[Sparse Signal 1D]: /blog/2014/05/18/sparse-signal-reconstruction-via-l1-minimization/
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Sparse Signal Reconstruction via L1-minimization]]></title>
    <link href="http://blog.ivansiu.com/blog/2014/05/18/sparse-signal-reconstruction-via-l1-minimization/"/>
    <updated>2014-05-18T22:13:08-05:00</updated>
    <id>http://blog.ivansiu.com/blog/2014/05/18/sparse-signal-reconstruction-via-l1-minimization</id>
    <content type="html"><![CDATA[[![Sparse Signal Reconstruction Results]][Sparse Signal Reconstruction Results]

This is a follow-up of the [previous post] on applications of L1 minimization.

<!-- more -->

As we know, any signal can be decomposed into a linear combination of basis,
and the most famous one is [Fourier Transform]. 
For simplicity, let's assume that we have a signal that
is a superposition of some sinusoids. For example, the following:

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="p">.</span><span class="mi">5</span><span class="o">*</span><span class="nb">sin</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">.*</span><span class="nb">cos</span><span class="p">(.</span><span class="mi">1</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="nb">sin</span><span class="p">(</span><span class="mf">1.3</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">.*</span><span class="nb">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="p">.</span><span class="mi">7</span><span class="o">*</span><span class="nb">sin</span><span class="p">(.</span><span class="mi">5</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">.*</span><span class="nb">cos</span><span class="p">(</span><span class="mf">2.3</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">.*</span><span class="nb">cos</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>

With *discrete consine transform* (DCT), we can easily find the coefficients of
corresponding sinusoid components. The above example's coefficients (in
frequency domain) and signal in time domain are shown in the post figure.

Now, let's assume we do not know the signal and want to reconstruct it
by sampling. Theorectically, the number of samples required is at least
two times the signal frequency, according to the famous
[Nyquist–Shannon sampling theorem].

However, this assume zero-knowledge about the signal. If we know some structure
of the signal, e.g., the DCT coefficients are sparse in our case, we can 
further reduce the number of samples required. [^Nsamples]

The following code snippet demonstrates how this works. We generate
the original signal in time domain and then perform a DCT to obtain the coefficients.

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="c">% sparse signal recovery using L1</span>
</span><span class='line'>
</span><span class='line'><span class="n">rng</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
</span><span class='line'><span class="n">N</span> <span class="p">=</span> <span class="mi">256</span><span class="p">;</span> <span class="n">R</span> <span class="p">=</span> <span class="mi">3</span><span class="p">;</span> <span class="n">C</span> <span class="p">=</span> <span class="mi">2</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="c">% some superposition of sinoisoids, feel free to change and experiment</span>
</span><span class='line'><span class="n">f</span> <span class="p">=</span> <span class="p">@(</span><span class="n">x</span><span class="p">)</span> <span class="p">.</span><span class="mi">5</span><span class="o">*</span><span class="nb">sin</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">.*</span><span class="nb">cos</span><span class="p">(.</span><span class="mi">1</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">+</span><span class="nb">sin</span><span class="p">(</span><span class="mf">1.3</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">.*</span><span class="nb">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="p">.</span><span class="mi">7</span><span class="o">*</span><span class="nb">sin</span><span class="p">(.</span><span class="mi">5</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">.*</span><span class="nb">cos</span><span class="p">(</span><span class="mf">2.3</span><span class="o">*</span><span class="n">x</span><span class="p">)</span><span class="o">.*</span><span class="nb">cos</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span><span class='line'><span class="n">x</span> <span class="p">=</span> <span class="nb">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="o">*</span><span class="nb">pi</span><span class="p">,</span> <span class="mi">10</span><span class="o">*</span><span class="nb">pi</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</span><span class='line'><span class="n">y</span> <span class="p">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">1</span><span class="p">);</span>
</span><span class='line'><span class="n">coef</span> <span class="p">=</span> <span class="n">dct</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">&#39;</span><span class="p">;</span>
</span><span class='line'><span class="n">stem</span><span class="p">(</span><span class="n">coef</span><span class="p">);</span>
</span><span class='line'><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span> <span class="n">N</span><span class="p">]);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;Original signal in frequency domain&#39;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">2</span><span class="p">);</span>
</span><span class='line'><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">);</span>
</span><span class='line'><span class="n">xlim</span><span class="p">([</span><span class="n">min</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">)]);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;Original signal in time domain&#39;</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>

Let's assume that we have a device that can sample from the frequency domain.
To do this, we create a *random measurement matrix* to obtain the samples.
We use 80 samples here. Note that we normalize the measurement matrix
to have orthonormal basis, i.e., the norm of each row is 1, and the dot product
of different row is 0.

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="c">% measurement matrix</span>
</span><span class='line'><span class="n">K</span><span class="p">=</span><span class="mi">80</span><span class="p">;</span>
</span><span class='line'><span class="n">A</span><span class="p">=</span><span class="nb">randn</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</span><span class='line'><span class="n">A</span><span class="p">=</span><span class="n">orth</span><span class="p">(</span><span class="n">A</span><span class="o">&#39;</span><span class="p">)</span><span class="o">&#39;</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="c">% observations</span>
</span><span class='line'><span class="n">b</span><span class="p">=</span><span class="n">A</span><span class="o">*</span><span class="n">coef</span><span class="p">;</span>
</span></code></pre></td></tr></table></div></figure>

We first try a least-square approach, which boils down to inverse the matrix
and obtain $\hat{x}=A^{-1} b$. Note that as A is not square, we are
using its *pseudo-inverse* here. Furthermore, as A is othornormal, its
transpose is the same as pseudo-inverse.

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="c">% min-energy observations</span>
</span><span class='line'><span class="n">c0</span> <span class="p">=</span> <span class="n">A</span><span class="o">&#39;*</span><span class="n">b</span><span class="p">;</span> <span class="c">% A&#39; = pinv(A) here since A is a full-rank orthonormal matrix</span>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">3</span><span class="p">);</span>
</span><span class='line'><span class="n">stem</span><span class="p">(</span><span class="n">c0</span><span class="p">);</span>
</span><span class='line'><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span> <span class="n">N</span><span class="p">]);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;Minimum energy recovery - coef&#39;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">4</span><span class="p">);</span>
</span><span class='line'><span class="n">y0</span> <span class="p">=</span> <span class="n">idct</span><span class="p">(</span><span class="n">c0</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</span><span class='line'><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span><span class="s">&#39;r&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="n">N</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">);</span>
</span><span class='line'><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span> <span class="n">N</span><span class="p">]);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;Minimum energy recovery - signal&#39;</span><span class="p">);</span>
</span><span class='line'><span class="n">legend</span><span class="p">(</span><span class="s">&#39;Recovered&#39;</span><span class="p">,</span> <span class="s">&#39;Original&#39;</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>

As we can see, there are lots of non-zeros in the coefficients, and
the recovered signal is very different from the original signal.

Finally, we use L1-minimization for reconstruction. I used `lasso` 
to perform a L1-regualarized minimization. Another package that performs various
L1-minimization is [l1-magic].

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="c">% L1-minimization</span>
</span><span class='line'><span class="p">[</span><span class="n">c1</span><span class="p">,</span> <span class="n">fitinfo</span><span class="p">]</span> <span class="p">=</span> <span class="n">lasso</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="s">&#39;Lambda&#39;</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">);</span>
</span><span class='line'><span class="c">% If use L1-magic</span>
</span><span class='line'><span class="c">% addpath /path/to/l1magic/Optimization</span>
</span><span class='line'><span class="c">% [c1] = l1eq_pd(c0, A, [], b, 1e-4);</span>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">5</span><span class="p">);</span>
</span><span class='line'><span class="n">stem</span><span class="p">(</span><span class="n">c1</span><span class="p">);</span>
</span><span class='line'><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span> <span class="n">N</span><span class="p">]);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;L1 recovery - coef&#39;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="n">R</span><span class="p">,</span><span class="n">C</span><span class="p">,</span><span class="mi">6</span><span class="p">);</span>
</span><span class='line'><span class="n">y1</span> <span class="p">=</span> <span class="n">idct</span><span class="p">(</span><span class="n">c1</span><span class="p">,</span> <span class="n">N</span><span class="p">);</span>
</span><span class='line'><span class="n">plot</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">N</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="n">N</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">);</span>
</span><span class='line'><span class="n">xlim</span><span class="p">([</span><span class="mi">0</span> <span class="n">N</span><span class="p">]);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;L1 recovery - signal&#39;</span><span class="p">);</span>
</span><span class='line'><span class="n">legend</span><span class="p">(</span><span class="s">&#39;Recovered&#39;</span><span class="p">,</span> <span class="s">&#39;Original&#39;</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>

The above shows that L1-minimization successfully recovered the original signal.
A complete code snippet can be found [here][gist].

[Sparse Signal Reconstruction Results]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/l1/sparse_signal_reconstruction.png
[Fourier Transform]: http://en.wikipedia.org/wiki/Fourier_transform
[Nyquist–Shannon sampling theorem]: http://en.wikipedia.org/wiki/Nyquist–Shannon_sampling_theorem
[l1-magic]: http://users.ece.gatech.edu/~justin/l1magic/
[gist]: https://gist.github.com/842145ae48381a5a8dca
[previous post]: /blog/2014/05/14/a-comparison-of-least-square-l2-regularization-and-l1-regularization/

[^Nsamples]: In order to recover f perfectly, we need at least $B \log (N)$ samples [(source)](http://users.ece.gatech.edu/~justin/l1magic/downloads/papers/CandesRombergTao_revisedNov2005.pdf).
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Comparison of Least Square, L2-regularization and L1-regularization]]></title>
    <link href="http://blog.ivansiu.com/blog/2014/05/14/a-comparison-of-least-square-l2-regularization-and-l1-regularization/"/>
    <updated>2014-05-14T21:32:38-05:00</updated>
    <id>http://blog.ivansiu.com/blog/2014/05/14/a-comparison-of-least-square-l2-regularization-and-l1-regularization</id>
    <content type="html"><![CDATA[![Recovered Coefficients by Different Methods][coef]

# Problem Setting

Ordinary Least Square (OLS), L2-regularization and L1-regularization are
all techniques of finding solutions in a linear system. However,
they serve for different purposes. Recently, L1-regularization gains
much attention due to its ability in finding sparse solutions.
This post demonstrates this by comparing OLS, L2 and L1 regularization.

<!-- more -->

Consider the following linear system:

$$Ax = y$$

where $A \in \reals^{m \times n}$, $m$ is the number of rows (observations) and
$n$ is the number of columns (variable dimension), $x$ is the variable
coefficients and $y$ is the response. There are three cases to consider:

1. $m=n$. This is the common-seen case. If $A$ is not degenerate, the solution
is unique.
2. $m>n$. This is called *over-determined linear system*. There is usually *no*
solutions, but an *approximation* can be easily found by minimizing
the *residue cost* $\norm{Ax-y}^2_2$ using least square methods, and it has
a nice closed-form solution $x_{ls}=(A^T A)^{-1} A^T y$. In L2-regularization,
we add a penalize term to minimize the 2-norm of the coefficients. Thus, the 
objective becomes:
$$\min_x \norm{Ax-y}^2_2 + \alpha \norm{x}_2$$
where $\alpha$ is a weight to decide the importance of the regularization.
3. $m<n$. This is called *under-determined linear system*. There is usually 
no solution or infinite solutions. This is where it get interesting: 
when we have some prior knowledge in the solution structure, such as
sparsity, we can have a 'metric' to find a better solution among a whole bunch.
The objective is thus:
$$\min_x \norm{Ax-y}^2_2 + \alpha \norm{x}_1$$
The optimization technique for the above problem is called [lasso], and there
is an advanced version called [elastic net], which [combines the L2 and L1
regularization together][combination], hoping to get the advantages of both: L1 regularization
finds sparse solution but introduces a large Mean Square Error (MSE) error,
while L2 is better at minimizing MSE. 

## An Example

In the following, we show their performances by solving a simple case.

<figure class='code'><figcaption><span>regression_ex.m</span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="c">% Compare Ordinary Least square (no regularization), L2-reguarlized (Ridge),</span>
</span><span class='line'><span class="c">% L1-regualarized (Lasso) regression in finding the sparse coefficient</span>
</span><span class='line'><span class="c">% in a underdetermined linear system</span>
</span><span class='line'>
</span><span class='line'><span class="n">rng</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>  <span class="c">% for reproducibility</span>
</span><span class='line'><span class="n">m</span> <span class="p">=</span> <span class="mi">50</span><span class="p">;</span>  <span class="c">% num samples</span>
</span><span class='line'><span class="n">n</span> <span class="p">=</span> <span class="mi">200</span><span class="p">;</span> <span class="c">% num variables, note that n &gt; m</span>
</span><span class='line'>
</span><span class='line'><span class="n">A</span> <span class="p">=</span> <span class="nb">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">);</span>
</span><span class='line'><span class="n">x</span> <span class="p">=</span> <span class="nb">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span><span class='line'><span class="n">nz</span> <span class="p">=</span> <span class="mi">10</span><span class="p">;</span> <span class="c">% 10 non-zeros variables (sparse)</span>
</span><span class='line'><span class="n">nz_idx</span> <span class="p">=</span> <span class="n">randperm</span><span class="p">(</span><span class="n">n</span><span class="p">);</span>
</span><span class='line'><span class="n">x</span><span class="p">(</span><span class="n">nz_idx</span><span class="p">(</span><span class="mi">1</span><span class="p">:</span><span class="n">nz</span><span class="p">))</span> <span class="p">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="nb">rand</span><span class="p">(</span><span class="n">nz</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span><span class='line'><span class="n">y</span> <span class="p">=</span> <span class="n">A</span><span class="o">*</span><span class="n">x</span><span class="p">;</span>
</span><span class='line'><span class="n">y</span> <span class="p">=</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="nb">rand</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="c">% add some noise</span>
</span><span class='line'>
</span><span class='line'><span class="c">% plot original x</span>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
</span><span class='line'><span class="n">bar</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">axis</span> <span class="n">tight</span><span class="p">;</span>
</span><span class='line'><span class="n">title</span><span class="p">(</span><span class="s">&#39;Original coefficients&#39;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="c">% OLS</span>
</span><span class='line'><span class="n">x_ols</span> <span class="p">=</span> <span class="n">A</span> <span class="o">\</span> <span class="n">y</span><span class="p">;</span>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">);</span>
</span><span class='line'><span class="n">bar</span><span class="p">(</span><span class="n">x_ols</span><span class="p">),</span> <span class="n">axis</span> <span class="n">tight</span><span class="p">;</span>
</span><span class='line'><span class="n">title</span><span class="p">(</span><span class="s">&#39;Ordinary Least Square&#39;</span><span class="p">);</span>
</span><span class='line'><span class="n">y_ols</span> <span class="p">=</span> <span class="n">A</span> <span class="o">*</span> <span class="n">x_ols</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="c">% L2 (Ridge) </span>
</span><span class='line'><span class="n">x_l2</span> <span class="p">=</span> <span class="n">ridge</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>  <span class="c">% last parameter = 00 to generate intercept term</span>
</span><span class='line'><span class="n">b_l2</span> <span class="p">=</span> <span class="n">x_l2</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</span><span class='line'><span class="n">x_l2</span> <span class="p">=</span> <span class="n">x_l2</span><span class="p">(</span><span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">);</span>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">);</span>
</span><span class='line'><span class="n">bar</span><span class="p">(</span><span class="n">x_l2</span><span class="p">),</span> <span class="n">axis</span> <span class="n">tight</span><span class="p">;</span>
</span><span class='line'><span class="n">title</span><span class="p">(</span><span class="s">&#39;L2 Regularization&#39;</span><span class="p">);</span>
</span><span class='line'><span class="n">y_l2</span> <span class="p">=</span> <span class="n">A</span> <span class="o">*</span> <span class="n">x_l2</span> <span class="o">+</span> <span class="n">b_l2</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="c">% L1 (Lasso)</span>
</span><span class='line'><span class="p">[</span><span class="n">x_l1</span><span class="p">,</span> <span class="n">fitinfo</span><span class="p">]</span> <span class="p">=</span> <span class="n">lasso</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">&#39;Lambda&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">);</span>
</span><span class='line'><span class="n">b_l1</span> <span class="p">=</span> <span class="n">fitinfo</span><span class="p">.</span><span class="n">Intercept</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</span><span class='line'><span class="n">y_l1</span> <span class="p">=</span> <span class="n">A</span> <span class="o">*</span> <span class="n">x_l1</span> <span class="o">+</span> <span class="n">b_l1</span><span class="p">;</span>
</span><span class='line'><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">);</span>
</span><span class='line'><span class="n">bar</span><span class="p">(</span><span class="n">x_l1</span><span class="p">),</span> <span class="n">axis</span> <span class="n">tight</span><span class="p">;</span>
</span><span class='line'><span class="n">title</span><span class="p">(</span><span class="s">&#39;L1 Regularization&#39;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="c">% L1 (Elastic Net)</span>
</span><span class='line'><span class="p">[</span><span class="n">x_en</span><span class="p">,</span> <span class="n">fitinfo_en</span><span class="p">]</span> <span class="p">=</span> <span class="n">lasso</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s">&#39;Lambda&#39;</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s">&#39;Alpha&#39;</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">);</span>
</span><span class='line'><span class="n">b_en</span> <span class="p">=</span> <span class="n">fitinfo_en</span><span class="p">.</span><span class="n">Intercept</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
</span><span class='line'><span class="n">y_en</span> <span class="p">=</span> <span class="n">A</span> <span class="o">*</span> <span class="n">x_en</span> <span class="o">+</span> <span class="n">b_en</span><span class="p">;</span>
</span><span class='line'>
</span><span class='line'><span class="n">MSE_y</span> <span class="p">=</span> <span class="p">[</span><span class="n">mse</span><span class="p">(</span><span class="n">y_ols</span><span class="o">-</span><span class="n">y</span><span class="p">),</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_l2</span><span class="o">-</span><span class="n">y</span><span class="p">),</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_l1</span><span class="o">-</span><span class="n">y</span><span class="p">),</span> <span class="n">mse</span><span class="p">(</span><span class="n">y_en</span><span class="o">-</span><span class="n">y</span><span class="p">)];</span>
</span><span class='line'><span class="nb">disp</span><span class="p">(</span><span class="s">&#39;Mean square error: &#39;</span><span class="p">)</span>
</span><span class='line'><span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;%g    &#39;</span><span class="p">,</span> <span class="n">MSE_y</span><span class="p">);</span> <span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;\n\n&#39;</span><span class="p">);</span>
</span><span class='line'>
</span><span class='line'><span class="c">% Plot the recovered coefficients</span>
</span><span class='line'><span class="n">figure</span><span class="p">,</span> <span class="n">hold</span> <span class="n">on</span>
</span><span class='line'><span class="n">plot</span><span class="p">(</span><span class="n">x_l1</span><span class="p">,</span> <span class="s">&#39;b&#39;</span><span class="p">);</span>
</span><span class='line'><span class="n">plot</span><span class="p">(</span><span class="n">x_en</span><span class="p">,</span> <span class="s">&#39;r&#39;</span><span class="p">);</span>
</span><span class='line'><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s">&#39;g--&#39;</span><span class="p">);</span>
</span><span class='line'><span class="n">legend</span><span class="p">(</span><span class="s">&#39;Lasso Coef&#39;</span><span class="p">,</span> <span class="s">&#39;Elastic Net coef&#39;</span><span class="p">,</span> <span class="s">&#39;Original Coef&#39;</span><span class="p">);</span>
</span></code></pre></td></tr></table></div></figure>

Output:

<figure class='code'><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class='matlab'><span class='line'><span class="n">Mean</span> <span class="n">square</span> <span class="n">error</span><span class="p">:</span>
</span><span class='line'><span class="mf">1.81793e-29</span>    <span class="mf">7.93494e-15</span>    <span class="mf">0.0975002</span>    <span class="mf">0.0641214</span>
</span></code></pre></td></tr></table></div></figure>

The above code snippets generates an under-determined matrix $A$, and
a sparse coefficients which has 200 variables but only 10 of them are non-zeros.
Noises are added to the responses. We then run the proposed three methods
to try to recover the coefficients. It then generates two plots:

1. The first plot is as shown in the top. As we can see, OLS does a very bad job,
though the MSE is minimized to zero. 
L2-regularization do find some of the sparks, but there are also lots of non-zeros
introduced. Finally, L1-regularization finds most of the non-zeros correctly
and resembles the original coefficients most.
2. The second plot shows how similar the recovered coefficients by lasso and
elastic nets resemble the original coefficients. As we can see, both of them
can recover most parts, while elastic nets contain some small 'noise'. However,
elastic net yields a slightly better MSE than lasso.

![Plot of Coefficients][plot]

## Probing Further

Scikit has some excellent examples on regualarization ([1], [2]). Quora has an
excellent [discussion] on L2 vs L1 regualarization. I found the top three answers
very useful in understanding deeper, especially from the *Bayesian regularization paradigm* perspective by thinking the regularization as MAP (Maximum A Posteriori)
that adds a Laplacian (L1) or Gaussian (L2) prior to the original objective.

[coef]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/2014-05-14-a-comparison-of-least-square-l2-regularization-and-l1-regularization/coef.png
[plot]: https://res.cloudinary.com/maomao/image/upload/v1491291930/blog/2014-05-14-a-comparison-of-least-square-l2-regularization-and-l1-regularization/plot.png
[lasso]: http://www-stat.stanford.edu/~tibs/lasso.html
[elastic net]: http://en.wikipedia.org/wiki/Elastic_net_regularization
[combination]: http://scikit-learn.org/stable/modules/linear_model.html#elastic-net
[1]: http://scikit-learn.org/stable/modules/linear_model.html
[2]: http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html
[discussion]: http://www.quora.com/Machine-Learning/What-is-the-difference-between-L1-and-L2-regularization
]]></content>
  </entry>
  
</feed>
