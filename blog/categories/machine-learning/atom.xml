<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Machine Learning | Zigang Xiao]]></title>
  <link href="http://blog.ivansiu.com/blog/categories/machine-learning/atom.xml" rel="self"/>
  <link href="http://blog.ivansiu.com/"/>
  <updated>2014-05-19T00:02:48-05:00</updated>
  <id>http://blog.ivansiu.com/</id>
  <author>
    <name><![CDATA[Zigang Xiao (Ivan)]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Sparse Signal Reconstruction via L1-minimization]]></title>
    <link href="http://blog.ivansiu.com/blog/2014/05/18/sparse-signal-reconstruction-via-l1-minimization/"/>
    <updated>2014-05-18T22:13:08-05:00</updated>
    <id>http://blog.ivansiu.com/blog/2014/05/18/sparse-signal-reconstruction-via-l1-minimization</id>
    <content type="html"><![CDATA[[![Sparse Signal Reconstruction Results]][Sparse Signal Reconstruction Results]

This is a follow-up of the [previous post] on applications of L1 minimization.

<!-- more -->

As we know, any signal can be decomposed into a linear combination of basis,
and the most famous one is [Fourier Transform]. 
For simplicity, let's assume that we have a signal that
is a superposition of some sinusoids. For example, the following:

``` matlab
.5*sin(3*x).*cos(.1*x)+sin(1.3*x).*sin(x)-.7*sin(.5*x).*cos(2.3*x).*cos(x);
```

With *discrete consine transform* (DCT), we can easily find the coefficients of
corresponding sinusoid components. The above example's coefficients (in
frequency domain) and signal in time domain are shown in the post figure.

Now, let's assume we do not know the signal and want to reconstruct it
by sampling. Theorectically, the number of samples required is at least
two times the signal frequency, according to the famous
[Nyquist–Shannon sampling theorem].

However, this assume zero-knowledge about the signal. If we know some structure
of the signal, e.g., the DCT coefficients are sparse in our case, we can 
further reduce the number of samples required. [^Nsamples]

The following code snippet demonstrates how this works. We generate
the original signal in time domain and then perform a DCT to obtain the coefficients.

``` matlab
% sparse signal recovery using L1

rng(0);
N = 256; R = 3; C = 2;

% some superposition of sinoisoids, feel free to change and experiment
f = @(x) .5*sin(3*x).*cos(.1*x)+sin(1.3*x).*sin(x)-.7*sin(.5*x).*cos(2.3*x).*cos(x);
x = linspace(-10*pi, 10*pi, N);
y = f(x);

subplot(R,C,1);
coef = dct(y)';
stem(coef);
xlim([0 N]); title('Original signal in frequency domain');

subplot(R,C,2);
plot(x,y);
xlim([min(x) max(x)]); title('Original signal in time domain');
```

Let's assume that we have a device that can sample from the frequency domain.
To do this, we create a *random measurement matrix* to obtain the samples.
We use 80 samples here. Note that we normalize the measurement matrix
to have orthonormal basis, i.e., the norm of each row is 1, and the dot product
of different row is 0.

``` matlab
% measurement matrix
K=80;
A=randn(K, N);
A=orth(A')';

% observations
b=A*coef;
```

We first try a least-square approach, which boils down to inverse the matrix
and obtain $\hat{x}=A^{-1} b$. Note that as A is not square, we are
using its *pseudo-inverse* here. Furthermore, as A is othornormal, its
transpose is the same as pseudo-inverse.

```
% min-energy observations
c0 = A'*b; % A' = pinv(A) here since A is a full-rank orthonormal matrix
subplot(R,C,3);
stem(c0);
xlim([0 N]); title('Minimum energy recovery - coef');

subplot(R,C,4);
y0 = idct(c0, N);
plot(1:N, y0,'r', 1:N, y, 'b');
xlim([0 N]); title('Minimum energy recovery - signal');
legend('Recovered', 'Original');
```

As we can see, there are lots of non-zeros in the coefficients, and
the recovered signal is very different from the original signal.

Finally, we use L1-minimization for reconstruction. I used `lasso` 
to perform a L1-regualarized minimization. Another package that performs various
L1-minimization is [l1-magic].

```
% L1-minimization
[c1, fitinfo] = lasso(A, b, 'Lambda', 0.01);
% If use L1-magic
% addpath /path/to/l1magic/Optimization
% [c1] = l1eq_pd(c0, A, [], b, 1e-4);
subplot(R,C,5);
stem(c1);
xlim([0 N]); title('L1 recovery - coef');

subplot(R,C,6);
y1 = idct(c1, N);
plot(1:N, y1, 'r', 1:N, y, 'b');
xlim([0 N]); title('L1 recovery - signal');
legend('Recovered', 'Original');
```

The above shows that L1-minimization successfully recovered the original signal.
A complete code snippet can be found [here][gist].

[Sparse Signal Reconstruction Results]: https://dl.dropboxusercontent.com/u/3315210/blog/l1/sparse_signal_reconstruction.png
[Fourier Transform]: http://en.wikipedia.org/wiki/Fourier_transform
[Nyquist–Shannon sampling theorem]: http://en.wikipedia.org/wiki/Nyquist–Shannon_sampling_theorem
[l1-magic]: http://users.ece.gatech.edu/~justin/l1magic/
[gist]: https://gist.github.com/842145ae48381a5a8dca
[previous post]: /blog/2014/05/14/a-comparison-of-least-square-l2-regularization-and-l1-regularization/

[^Nsamples]: In order to recover f perfectly, we need at least $B \log (N)$ samples [(source)](http://users.ece.gatech.edu/~justin/l1magic/downloads/papers/CandesRombergTao_revisedNov2005.pdf).]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[A Comparison of Least Square, L2-regularization and L1-regularization]]></title>
    <link href="http://blog.ivansiu.com/blog/2014/05/14/a-comparison-of-least-square-l2-regularization-and-l1-regularization/"/>
    <updated>2014-05-14T21:32:38-05:00</updated>
    <id>http://blog.ivansiu.com/blog/2014/05/14/a-comparison-of-least-square-l2-regularization-and-l1-regularization</id>
    <content type="html"><![CDATA[![Recovered Coefficients by Different Methods][coef]

# Problem Setting

Ordinary Least Square (OLS), L2-regularization and L1-regularization are
all techniques of finding solutions in a linear system. However,
they serve for different purposes. Recently, L1-regularization gains
much attention due to its ability in finding sparse solutions.
This post demonstrates this by comparing OLS, L2 and L1 regularization.

<!-- more -->

Consider the following linear system:

$$Ax = y$$

where $A \in \reals^{m \times n}$, $m$ is the number of rows (observations) and
$n$ is the number of columns (variable dimension), $x$ is the variable
coefficients and $y$ is the response. There are three cases to consider:

1. $m=n$. This is the common-seen case. If $A$ is not degenerate, the solution
is unique.
2. $m>n$. This is called *over-determined linear system*. There is usually *no*
solutions, but an *approximation* can be easily found by minimizing
the *residue cost* $\norm{Ax-y}^2_2$ using least square methods, and it has
a nice closed-form solution $x_{ls}=(A^T A)^{-1} A^T y$. In L2-regularization,
we add a penalize term to minimize the 2-norm of the coefficients. Thus, the 
objective becomes:
$$\min_x \norm{Ax-y}^2_2 + \alpha \norm{x}_2$$
where $\alpha$ is a weight to decide the importance of the regularization.
3. $m<n$. This is called *under-determined linear system*. There is usually 
no solution or infinite solutions. This is where it get interesting: 
when we have some prior knowledge in the solution structure, such as
sparsity, we can have a 'metric' to find a better solution among a whole bunch.
The objective is thus:
$$\min_x \norm{Ax-y}^2_2 + \alpha \norm{x}_1$$
The optimization technique for the above problem is called [lasso], and there
is an advanced version called [elastic net], which [combines the L2 and L1
regularization together][combination], hoping to get the advantages of both: L1 regularization
finds sparse solution but introduces a large Mean Square Error (MSE) error,
while L2 is better at minimizing MSE. 

## An Example

In the following, we show their performances by solving a simple case.

``` matlab regression_ex.m
% Compare Ordinary Least square (no regularization), L2-reguarlized (Ridge),
% L1-regualarized (Lasso) regression in finding the sparse coefficient
% in a underdetermined linear system

rng(0);  % for reproducibility
m = 50;  % num samples
n = 200; % num variables, note that n > m

A = rand(m, n);
x = zeros(n, 1);
nz = 10; % 10 non-zeros variables (sparse)
nz_idx = randperm(n);
x(nz_idx(1:nz)) = 3 * rand(nz, 1);
y = A*x;
y = y + 0.05 * rand(m, 1); % add some noise

% plot original x
subplot(2, 2, 1);
bar(x), axis tight;
title('Original coefficients');

% OLS
x_ols = A \ y;
subplot(2, 2, 2);
bar(x_ols), axis tight;
title('Ordinary Least Square');
y_ols = A * x_ols;

% L2 (Ridge) 
x_l2 = ridge(y, A, 1e-5, 0);  % last parameter = 00 to generate intercept term
b_l2 = x_l2(1);
x_l2 = x_l2(2:end);
subplot(2, 2, 3);
bar(x_l2), axis tight;
title('L2 Regularization');
y_l2 = A * x_l2 + b_l2;

% L1 (Lasso)
[x_l1, fitinfo] = lasso(A, y, 'Lambda', 0.1);
b_l1 = fitinfo.Intercept(1);
y_l1 = A * x_l1 + b_l1;
subplot(2, 2, 4);
bar(x_l1), axis tight;
title('L1 Regularization');

% L1 (Elastic Net)
[x_en, fitinfo_en] = lasso(A, y, 'Lambda', 0.1, 'Alpha', 0.7);
b_en = fitinfo_en.Intercept(1);
y_en = A * x_en + b_en;

MSE_y = [mse(y_ols-y), mse(y_l2-y), mse(y_l1-y), mse(y_en-y)];
disp('Mean square error: ')
fprintf('%g    ', MSE_y); fprintf('\n\n');

% Plot the recovered coefficients
figure, hold on
plot(x_l1, 'b');
plot(x_en, 'r');
plot(x, 'g--');
legend('Lasso Coef', 'Elastic Net coef', 'Original Coef');
```

Output:

``` matlab
Mean square error: 
1.81793e-29    7.93494e-15    0.0975002    0.0641214    
```

The above code snippets generates an under-determined matrix $A$, and
a sparse coefficients which has 200 variables but only 10 of them are non-zeros.
Noises are added to the responses. We then run the proposed three methods
to try to recover the coefficients. It then generates two plots:

1. The first plot is as shown in the top. As we can see, OLS does a very bad job,
though the MSE is minimized to zero. 
L2-regularization do find some of the sparks, but there are also lots of non-zeros
introduced. Finally, L1-regularization finds most of the non-zeros correctly
and resembles the original coefficients most.
2. The second plot shows how similar the recovered coefficients by lasso and
elastic nets resemble the original coefficients. As we can see, both of them
can recover most parts, while elastic nets contain some small 'noise'. However,
elastic net yields a slightly better MSE than lasso.

![Plot of Coefficients][plot]

## Probing Further

Scikit has some excellent examples on regualarization ([1], [2]). Quora has an
excellent [discussion] on L2 vs L1 regualarization. I found the top three answers
very useful in understanding deeper, especially from the *Bayesian regularization paradigm* perspective by thinking the regularization as MAP (Maximum A Posteriori)
that adds a Laplacian (L1) or Gaussian (L2) prior to the original objective.

[coef]: https://dl.dropboxusercontent.com/u/3315210/blog/2014-05-14-a-comparison-of-least-square-l2-regularization-and-l1-regularization/coef.png
[plot]: https://dl.dropboxusercontent.com/u/3315210/blog/2014-05-14-a-comparison-of-least-square-l2-regularization-and-l1-regularization/plot.png
[lasso]: http://www-stat.stanford.edu/~tibs/lasso.html
[elastic net]: http://en.wikipedia.org/wiki/Elastic_net_regularization
[combination]: http://scikit-learn.org/stable/modules/linear_model.html#elastic-net
[1]: http://scikit-learn.org/stable/modules/linear_model.html
[2]: http://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_and_elasticnet.html
[discussion]: http://www.quora.com/Machine-Learning/What-is-the-difference-between-L1-and-L2-regularization]]></content>
  </entry>
  
</feed>
