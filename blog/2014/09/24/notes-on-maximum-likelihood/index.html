
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Notes on Maximum Likelihood, Maximum A Posteriori and Naive Bayes - Ivan's Blog</title>
  <meta name="author" content="Zigang "Ivan" Xiao">
  <link rel="author" href="humans.txt">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

  
    
  
  <meta name="description" content="Let \(\data\) be a set of data generated from some distribution parameterized by \(\theta\). We want to estimate the unknown parameter \(\theta\). &hellip;">
  
  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://blog.ivansiu.com/blog/2014/09/24/notes-on-maximum-likelihood/">
  <link href='http://fonts.googleapis.com/css?family=Cantarell' rel='stylesheet' type='text/css'>
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Ivan's Blog" type="application/atom+xml">
  <meta name="og:type" content="website" />
  <meta name="og:site_name" content="Ivan's Blog" />
  <meta name="og:title" content="Notes on Maximum Likelihood, Maximum A Posteriori and Naive Bayes" />
  <meta name="og:description" content="Let \(\data\) be a set of data generated from some distribution parameterized by \(\theta\). We want to estimate the unknown parameter \(\theta\). &hellip;" />
  <meta name="og:url" content="http://blog.ivansiu.com/blog/2014/09/24/notes-on-maximum-likelihood/"/>
  <meta name="url" content="http://blog.ivansiu.com/blog/2014/09/24/notes-on-maximum-likelihood/">
  
  <meta name="subject" content="machine learningprobability"/>
  <meta name="category" content="machine learningprobability"/>
  
  <meta name="distribution" content="global">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

<!-- MathJax -->
<!-- see:

http://www.idryman.org/blog/2012/03/10/writing-math-equations-on-octopress/ 
http://drz.ac/2013/01/03/blogging-with-math/
http://drz.ac/javascripts/MathJaxLocal.js
http://blog.zhengdong.me/2012/12/19/latex-math-in-octopress
http://docs.mathjax.org/en/latest/configuration.html

-->

<script type="text/javascript"
	src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

<script type="text/javascript" src="/javascripts/MathJaxLocal.js"></script>

<!-- Facebook OpenGraph -->
<meta property="og:image" content="http://blog.ivansiu.com/favicon.png">

<!-- Fav icon 
see: 
https://github.com/imathis/octopress/issues/599
-->


  

  
<link href="/favicon.ico" rel="icon">
</head>

<body   >
  <nav role="navigation"><div class="navbar">
  <div class="navbar-inner">
    <a class="brand" href="/">Ivan's Blog</a>
    <ul class="nav">
      <li><a href="/">Home</a></li>
      <li><a href="/blog/archives">Archives</a></li>
    </ul>
        <ul class="nav">
      <li><a href="/blog/categories">Categories</a></li>
    </ul>

    <ul class="nav" data-subscription="rss">
      <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
      
    </ul>
      
    <form class="navbar-form" action="http://google.com/search" method="get">
      <fieldset role="search">
        <input type="hidden" name="q" value="site:blog.ivansiu.com" />
        <input class="span2" type="text" name="q" results="0" placeholder="Search"/>
      </fieldset>
    </form>
      
  </div>
</div>
</nav>
  <div class="wrapper_single">
    <div class="container">
      <article class="span8 offset2" role="article">
        <div class="article-format">

  <header>
    
      <h1 class="entry-title">Notes on Maximum Likelihood, Maximum a Posteriori and Naive Bayes</h1>
    
    
      <p class="meta">
        
  


  
    <span class="byline author vcard">by <span class="fn">Zigang "Ivan" Xiao</span></span>
  

 - 
        








  


<time datetime="2014-09-24T13:41:44-07:00" pubdate data-updated="true"></time> - 
        

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/machine-learning/'>machine learning</a>, <a class='category' href='/blog/categories/probability/'>probability</a>
  
</span>


        
      </p>
    
  </header>


  <div class="entry-content"><p>Let <span class="math inline">\(\data\)</span> be a set of data generated from some distribution parameterized by <span class="math inline">\(\theta\)</span>. We want to <em>estimate</em> the unknown parameter <span class="math inline">\(\theta\)</span>. What we can do?</p>
<!-- more -->
<p>Essentially, we want to find a most likely value of <span class="math inline">\(\theta\)</span> given <span class="math inline">\(\data\)</span>, that is <span class="math inline">\(\arg \max P(\theta | \data)\)</span>. According to Bayes Rule, we have</p>
<p><span class="math display">\[
P(\theta \given \data) = \frac{P(\data \given \theta)P(\theta)}{P(\data)}
\]</span></p>
<p>and the terms have the following meanings:</p>
<ul>
<li><span class="math inline">\(P(\theta \given \data)\)</span>: Posterior</li>
<li><span class="math inline">\(P(\data \given \theta)\)</span>: Likelihood</li>
<li><span class="math inline">\(P(\theta)\)</span>: Prior</li>
<li><span class="math inline">\(P(\data)\)</span>: Evidence</li>
</ul>
<h2 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h2>
<p>An easy way out is to use the MLE method. We want to find a <span class="math inline">\(\theta\)</span> the <em>best explains</em> the data. That is, we maximize <span class="math inline">\(P(\data \given \theta)\)</span>. Denote such a value as <span class="math inline">\(\hat{\theta}_{ML}\)</span>. We have</p>
<p><span class="math display">\[
\hat{\theta}_{ML} = \argmax_\theta P(\data \given \theta) =
\argmax_\theta P(\mathbf{x}_1, \ldots, \mathbf{x}_N \given \theta )
\]</span></p>
<p>Note that the above <span class="math inline">\(P\)</span> is a joint distribution over the data. We usually assume the observations are <em>independent</em>. Thus, we have</p>
<p><span class="math display">\[
P(\mathbf{x}_1, \ldots, \mathbf{x}_N \given \theta ) =
\prod_{i=1}^{N} P(\mathbf{x}_i \given \theta )
\]</span></p>
<p>We usually use logarithm to simplify the computation, as logarithm is monotonically increasing. Thus, we write:</p>
<p><span class="math display">\[
\mathcal{L}(\data \given \theta) = \sum_{i=1}^N \log P(\mathbf{x}_i \given \theta )
\]</span></p>
<p>Finally, we seek for the ML solution:</p>
<p><span class="math display">\[
\hat{\theta}_{ML} = \argmax_\theta \mathcal{L}(\data \given \theta)
\]</span></p>
<p>If we know the distribution <span class="math inline">\(P\)</span>, we can usually solve the above by setting derivative of <span class="math inline">\(\theta\)</span> to 0 and solve for <span class="math inline">\(\theta\)</span>, that is,</p>
<p><span class="math display">\[
\frac{\partial L}{\partial \theta} = 0
\]</span></p>
<h2 id="maximum-a-posteriori-map">Maximum A Posteriori (MAP)</h2>
<p>In MAP, we maximize <span class="math inline">\(P(\theta \given \data)\)</span> directly. Denote the MAP hypothesis as <span class="math inline">\(\hat{\theta}_{MAP}\)</span>, we have:</p>
<p><span class="math display">\[\begin{array}{rl}
\hat{\theta}_{MAP} = &amp; \argmax_\theta P(\theta \given \data) \\\\
 = &amp; \argmax_\theta \frac{P(\data \given \theta)P(\theta)}{P(\data)} \\\\
 = &amp; \argmax_\theta P(\data \given \theta)P(\theta)
\end{array}\]</span></p>
<p>Note that the last step is due to the evidence (data) <span class="math inline">\(\data\)</span> is constant, and thus can be omitted in <span class="math inline">\(\argmax\)</span>.</p>
<p>At this step, we notice that the only difference between <span class="math inline">\(\hat{\theta}_{ML}\)</span> and <span class="math inline">\(\hat{\theta}_{MAP}\)</span> is the prior term <span class="math inline">\(P(\theta)\)</span>. Another way to interpret is that we consider <span class="math inline">\(MAP\)</span> is more general than <span class="math inline">\(MLE\)</span>, as if we assume all the possible <span class="math inline">\(\theta\)</span> are equally probable a priori, e.g., they have the same prior probability, or <em>uniform prior</em>, we can effectively remove <span class="math inline">\(P(\theta)\)</span> from the MAP formula, and it looks like exactly the same as MLE.</p>
<p>Finally, if the independent observation holds, again we can use logarithm and expand <span class="math inline">\(\hat{\theta}_{MAP}\)</span> as:</p>
<p><span class="math display">\[
\begin{array}{rl}
\hat{\theta}_{MAP} = &amp; \argmax_\theta L(\data \given \theta) \\\\
 = &amp; \argmax_\theta \sum_{i=1}^{N} \log P(\mathbf{x}_i \given \theta ) + \log P(\theta)
\end{array}
\]</span></p>
<p>The extra prior term has the effect that we are essentially ‘pulling’ the <span class="math inline">\(\theta\)</span> distribution towards prior value. This makes sense as we are putting our domain knowledge as <em>prior</em> and intuitively the estimation is biased towards the <em>prior</em> value.</p>
<h2 id="naive-bayes-classifier">Naive Bayes Classifier</h2>
<p>Assume that we are given a set of data <span class="math inline">\(\data\)</span>, where each example <span class="math inline">\(\mathbf{x_j}=(a_1, a_2, \ldots, a_n)\)</span>, which can be viewed as <em>conjunctions of attributes values</em>. <span class="math inline">\(v_j \in V\)</span> is the corresponding class value. Using MAP, we can classify an example <span class="math inline">\(\mathbf{x}\)</span> as:</p>
<p><span class="math display">\[v_{MAP}=\argmax_{v_j\in V} P(v_j \given a_1, \ldots, a_n)\]</span></p>
<p>The problem is that it is hard to find a joint distribution for <span class="math inline">\(P(\mathbf{x} \given \theta)\)</span>. If we use the data to estimate the distribution, we typically don’t have enough data for each attribute. In other words, the data we have is very sparse compared to the whole distribution space.</p>
<p>Naive bayes makes the assumption that each attribute is <em>conditionally independent</em> given the target class <span class="math inline">\(v_j\)</span>, that is,</p>
<p><span class="math display">\[P(a_1, \ldots, a_n \given v_j) = \prod_{i=1}^n P(a_i \given v_j)\]</span></p>
<p>which can be easily estimated from the data. Thus, we have the following naive bayes classifier:</p>
<p><span class="math display">\[v_{NB} = \argmax_{v_j \in V} P(v_j) \prod_{i=1}^n P(a_i \given v_j)\]</span></p>
<p>Note that the learning of naive bayes simply involves in estimating <span class="math inline">\(P(a_i \given v_j)\)</span> and <span class="math inline">\(P(v_j)\)</span> based on the frequencies in the training data.</p>
<p>Normally the conditional independence assumption does not hold, but naive bayes performs well even if so. More importantly, <strong>when conditional independence is satisfied, Naive Bayes corresponds to MAP classification.</strong></p>
<h2 id="conclusion">Conclusion</h2>
<p>MLE, MAP and Naive Bayes are all connected. While MLE and MAP are parameter estimation methods that returns a single value of the paramter being estimated, NB is a classifier that predicts the probability of the class that an example belongs to. We also have the following insightes:</p>
<ul>
<li>Given the data, MLE considers the paramter to be a constant and estimates a value that provide maximum support for the data.</li>
<li>MLE does not allow us to ‘inject’ our beliefs about the likely values for the parameter (prior) in the estimation process.</li>
<li>MAP allows the fact that the paramter can take values from a prior (non-uniform) distribution that express our prior beliefs regarding the paramters.</li>
<li>MAP returns paramter value where the probability is highest given data.</li>
<li>Again, both MLE and MAP returns a single and specific value for the paramter. By contrast, <em>bayesian estimation</em> computes the full posterior distribution <span class="math inline">\(P(\theta \given \data)\)</span>.</li>
</ul>
<h2 id="thoughts">Thoughts</h2>
<p>After reading this <a href="http://www.behind-the-enemy-lines.com/2008/01/are-you-bayesian-or-frequentist-or.html">article</a>, I have the following interpretation:</p>
<ul>
<li>The Maximum Likelihood approach can be roughly regarded as traditional “frequentist” thinking.</li>
<li>The MAP approach is a direct applicatin of Bayes Theorem. Thus, it can be regarded as a “bayesian” way of thinking.</li>
</ul>
</div>


  <footer>
    <p class="meta">
      
  


  
    <span class="byline author vcard">by <span class="fn">Zigang "Ivan" Xiao</span></span>
  

 - 
      








  


<time datetime="2014-09-24T13:41:44-07:00" pubdate data-updated="true"></time> - 
      

posted in
<span class="categories">
  
    <a class='category' href='/blog/categories/machine-learning/'>machine learning</a>, <a class='category' href='/blog/categories/probability/'>probability</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="//twitter.com/share" class="twitter-share-button" data-url="http://blog.ivansiu.com/blog/2014/09/24/notes-on-maximum-likelihood/" data-via="iveney" data-counturl="http://blog.ivansiu.com/blog/2014/09/24/notes-on-maximum-likelihood/" >Tweet</a>
  
  
  
    <div class="fb-like" data-layout="button_count" data-send="true" data-width="450" data-show-faces="false" style="vertical-align: top;"></div>
  
</div>

    
    
      <a class="pull-left" href="/blog/2014/05/28/fix-mid-2009-mbp-ram-not-recognized-issue/" title="Previous Post: Fix Mid 2009 MBP RAM not recognized issue">&laquo; Fix Mid 2009 MBP RAM not recognized issue</a>
    
    
      <a class="pull-right" href="/blog/2014/10/01/jing-ying-zhu-yi-yu-tou-piao-kao-shi/" title="Next Post: 精英主义与投票考试">精英主义与投票考试 &raquo;</a>
    
  </footer>

</div>

        
          <div class="article-format">
            <h1>Comments</h1>
            <div id="disqus_thread" aria-live="polite"><noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>
          </div>
        
      </article>
    </div>
  </div>
  <div id="footer-widgets">
  <div class="container">
    <div class="row">
<!--
  <div class="span3">
    <h2>recent posts</h2>
    <ul class="recent_posts">
      
        <li>
          <a href="/blog/2017/08/06/xia-wei-yi-you-ji-(si-)-chi-chi-he-he/">夏威夷遊記（四） 吃吃喝喝</a>
        </li>
      
        <li>
          <a href="/blog/2017/08/06/xia-wei-yi-you-ji-(san-)-mao-yi-dao-molokini-snorkelling/">夏威夷遊記（三） 茂宜島 Molokini Snorkelling</a>
        </li>
      
        <li>
          <a href="/blog/2017/08/05/xia-wei-yi-you-ji-(er-)-mao-yi-dao/">夏威夷遊記（二） 茂宜島 Road to Hana, Ahihi-Kinau, Lahaina</a>
        </li>
      
        <li>
          <a href="/blog/2017/08/05/xia-wei-yi-you-ji-yi-wa-hu-dao/">夏威夷遊記（一） 瓦胡島</a>
        </li>
      
        <li>
          <a href="/blog/2017/08/01/zu-che-xian-jing-:toll-charges/">美國租車陷阱：Toll Charges</a>
        </li>
      
    </ul>
    <h2><a href="/blog/archives">archives</a></h2>
  </div>
  <div class="span3">
    

  </div>
  <div class="span4">
    <h2>twitter</h2>
<a class="twitter-timeline" href="https://twitter.com/iveney" data-widget-id="390943167203127296">Tweets by @iveney</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+"://platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

  </div>
  <div class="span2">
    <h2>found on</h2>

<a href="https://github.com/iveney/" rel="tooltip" title="Github"><img class="social_icon" title="Github" alt="github icon" src="/images/glyphicons_381_github.png"></a>



<a href="http://stackoverflow.com/users/577704/" rel="tooltip" title="Stack Overflow"><img class="social_icon" title="Stack Overflow" alt="stack overflow icon" src="/images/glyphicons_400_stack_overflow.png"></a>



<a href="http://www.linkedin.com/in/zgxiao" rel="tooltip" title="Linkedin"><img class="social_icon" title="Linkedin" alt="Linkedin icon" src="/images/glyphicons_377_linked_in.png"></a>



<a href="http://twitter.com/iveney" rel="tooltip" title="Twitter"><img class="social_icon" title="Twitter" alt="Twitter icon" src="/images/glyphicons_391_twitter_t.png"></a>











<a href="http://ivansiu.com" rel="tooltip" title="About"><img class="social_icon" title="About" alt="About icon" src="/images/glyphicons_003_user.png"></a>


  </div>
-->
</div>

  </div>
</div>
<footer class="footer">
  <div class="container">
    <p class="pull-left">
  <a href="/">Ivan's Blog</a>
  - Copyright &copy; 2017 - Zigang "Ivan" Xiao
</p>
<p class="pull-right">
  <span>Powered by <a href="http://octopress.org/">Octopress</a>.</span>
  
    <span>Designed by <a href="http://www.AdrianArtiles.com">Adrian Artiles</a>.</span>
  
</p>

  </div>
</footer>

  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.7.2/jquery.min.js" type="text/javascript"></script>
<script>window.jQuery || document.write('<script src="/javascripts/libs/jquery-1.7.2.min.js" type="text/javascript"><\/script>')</script>
<script src="/javascripts/libs/bootstrap.min.js" type="text/javascript"></script>
<script src="/javascripts/jquery.tweet.js" type="text/javascript"></script>
<script src="/javascripts/jquery.instagram.js" type="text/javascript"></script>
<script src="/javascripts/libs/jquery.masonry.min.js" type="text/javascript"></script>
<script src="/javascripts/custom.js" type="text/javascript"></script>


<script type="text/javascript">
      var disqus_shortname = 'iveney';
      
        
        // var disqus_developer = 1;
        var disqus_identifier = 'http://blog.ivansiu.com/blog/2014/09/24/notes-on-maximum-likelihood/';
        var disqus_url = 'http://blog.ivansiu.com/blog/2014/09/24/notes-on-maximum-likelihood/';
        var disqus_script = 'embed.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>





  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>




</body>
</html>
